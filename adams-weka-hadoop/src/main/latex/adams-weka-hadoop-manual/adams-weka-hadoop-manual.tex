% Copyright (c) 2012 by the University of Waikato, Hamilton, NZ. 
% This work is made available under the terms of the 
% Creative Commons Attribution-ShareAlike 4.0 license,
% http://creativecommons.org/licenses/by-sa/4.0/.
%
% Version: $Revision$

\documentclass[a4paper]{book}

\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{scalefnt}
\usepackage{tikz}

% watermark -- for draft stage
\usepackage[firstpage]{draftwatermark}
\SetWatermarkLightness{0.9}
\SetWatermarkScale{5}

\input{latex_extensions}

\title{
  \textbf{ADAMS} \\
  {\Large \textbf{A}dvanced \textbf{D}ata mining \textbf{A}nd \textbf{M}achine
  learning \textbf{S}ystem} \\
  {\Large Module: adams-weka-hadoop} \\
  \vspace{1cm}
  \includegraphics[width=4cm]{images/hadoop.png} \\
}
\author{
  Zufeng Yu \\
  Peter Reutemann
}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

\begin{document}

\begin{titlepage}
\maketitle

\thispagestyle{empty}
\center
\begin{table}[b]
	\begin{tabular}{c l l}
		\parbox[c][2cm]{2cm}{\copyright 2012-2013} &
		\parbox[c][2cm]{5cm}{\includegraphics[width=5cm]{images/coat_of_arms.pdf}} \\
	\end{tabular}
	\includegraphics[width=12cm]{images/cc.png} \\
\end{table}

\end{titlepage}

\tableofcontents
\listoffigures
%\listoftables

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\noindent This manual includes all details regarding to run Hadoop experiments
on both command line and Hadoop Experimenter from ADAM. It contains two major
sections which are \textbf{Running From Command Line} and \textbf{Running from Hadoop Gui
Experimenter}. There are two more sections afterwards which are \textbf{Tips on
Hadoop Cluster setting} and \textbf{Summary}.
\chapter{Running From Command Line}
\noindent In order to run weka experiments using hadoop from command line, there
are 11 options have to be specified. The complete command line should look
liks this:

{\scriptsize
\begin{verbatim}
	   bin/hadoop	    
	   Must be under hadoop directory
	   
	   --config    		
	   Path of Hadoop configuration folder, etc /home/z123/hadoop-0.20.2/conf.
	   					        		
   jar         		  
   The jar file generated by Hadoop Gui Experimenter on the fly. For example,
   jar hadoopGui4812493.jar.
       
   -libjars    		  
   All jars on classpath. For example, -libjars a.jar,b.jar,c.jar

   -dataset		      
   The path of an input dataset, can be used multiple times to input datasets.
   For example, -datasets /home/datasets/a.arff -datasets /home/datasets/b.arff
       					 
   -classifier 	      
   The path of an input classifier, can be used multiple times to input
   classifiers. For example, -classifier weka.classifiers.functions.SMO
   -classifier weka.classifiers.classifiers.trees.J48
       
   -runs			  	
   Number of repetition, etc -runs 10
       
   -folds			  	
   Number of folds, etc -folds 10
       
   -exptype			  	
   Choice of {classification,regression}. Etc -exptype classification
       
   -classindex		  	
   Choice of {last,first,default,an integer}. For example, -classindex last, or
   -classindex 5, or -classindex default
       					
   -confhome			
   Path of the hadoop conf folder. The input of this option must be exactly the
   same as --config.
       
   -csv					
   Experiment output file path. Note that an arff file with same path and name
   will be generated at same time. For example, by setting -csv /home/temp.csv,
   you will get temp.csv and temp.arff under /home.
\end{verbatim}
}
\noindent Every command option must be filled with correct input. When you run
experiment on ADAMS using Hadoop experiment, it will generate a complete command line string each time it starts
the hadoop experiment, we strongly suggest you to copy the full command line 
string instead of writing your own.\\

\noindent The jar file that you need is generated on the fly. You have to start
the experiment on ADAMS using Hadoop Experiment, and the jar file will be created under hadoop
home directory you have chosen. Note that there is no need to complete 
the experiment, because the program will create a jar file before everything
else starts running, you can abort the experiment as soon as you get the jar file.
The name and path of jar file will be shown on the panel.\\

\noindent Once you have the jar file, you can start running experiment from
command line. The first three command options have to stay the exact order as shown
above, which are \texttt{hadoop --config \ldots jar \ldots -libjars \ldots}. 
And rest of the options are not restricted in order, but it is compulsory 
to provide input values to all the options.

\chapter{Running From Hadoop Gui Experimenter}
\noindent After you start up Hadoop Experiment from ADAM, you will see a Gui
interface similar with normal Weka experimenter. However there are a few changes
need to pay attention to.\\

\noindent There are only two tabbed pannel instead of 3. We have removed the
result pannel.\\

\noindent In \textbf{Setup} tab, under \textbf{Path Setting} section, it allows
you to choose the hadoop home directory, specific hadoop configuration folder and the
output file path. You can choose different versions of hadoop home directory,
and each hadoop configuration may represent different cluster settings. Note that current
experiment will rely on the configuration setting you have chosen. Regarding
to output file path, it only asks you to give a path for CSV file, and
the program will generate an arff file with same name/path in the end.\\

\noindent Hadoop Experimenter only performs Cross-Validation experiments, and
loops iteration control will always be datasets first.\\

\noindent In \textbf{Run} tab, after you click on Start button, the
experiment won't start until the program successfully create a jar file for
current experiment.

\chapter{Tips on Hadoop Cluster}
This chapter describes a few problems that might occur while running Hadoop
Experiment on cluster. For more information please read Hadoop: The Definitive
Guide [5], or Pro Hadoop [6].

\heading{A. Abort hadoop experiment}

\noindent If a runnin hadoop experiment is interrupted half-way, it is necessary
to run this command in command window: \texttt{hadoop --config \ldots/conf job -kill currentJobId}.
This command will kill all ongoing hadoop child process in the specified
cluster for current Job. Job Id was shown on the panel when experiment
started.

\heading{B. Java heap size error}

\noindent By default hadoop only gives around 200m heap size to each task. This
error might occur if you are running regression algorithms, such as SMOreg. The best
solution so far is to increase Java heap size for each task. In the
configuration folder, modify \texttt{conf/mapred-site.xml} file with few more
lines:
\begin{verbatim}
<property>
    <name>mapred.child.java.opts</name>
    <value>-Xmx512m</value>
  </property>
\end{verbatim}

\noindent It sets heap size to 512m, feel free to increase the size if
necessary.

\heading{C. java.net.SocketTimeoutException: 480000 millis timeout}

\noindent This bug sometimes occurs while running large experiment. It seems to
be an I/O issue, and you can see the message in the tasktracker log files. However the
only possible solution so far is to add following lines into conf/hdfs-site.xml
file:
\begin{verbatim}
<property>
    <name>dfs.datanode.socket.write.timeout</name>
    <value>0</value>
</property>
\end{verbatim}


\heading{D. Start up different clusters using \texttt{--config \ldots/conf}}

\noindent Normally you can use \texttt{bin/start-dfs.sh} and
\texttt{bin/start-mapred.sh} to start up a hadoop cluster, if you add \texttt{ --config \ldots/conf} after these
commands, it will start the specific hadoop cluster according to the conf setting. For example,
\texttt{bin/start-dfs.sh --config clusterA/conf, bin/start-mapred.sh --config
clusterA/conf},now you have clusterA running, and then you can do
\texttt{bin/start-dfs.sh --config clusterB/conf, bin/start-mapred.sh --config
clusterB/conf}, and now you have clusterB running as well. Use \texttt{hadoop
--config \ldots/conf dfsadmin -report} to check if you have the HDFS running.
Also you can check log files in log folder to see if everything works fine.
There are a few types of log files, and the files worth checking are related to
namenodes, datanode, jobtracker and tasktracker.

\heading{E. log file errors}\\
\noindent Sometimes hadoop experimenter reports error in log files. One way to fix it is to find out
on which node has the problem. Then log in to
that machine, under hadoop home directory, if it's tasktracker' problem thenrun
\texttt {bin/hadoop-daemon.sh --config \ldots/conf start tasktracker}. It will start
tasktracker on this individual machine, then it becomes part of the cluster
based on your configuration folder setting. If it is the datanode problem, then
do \texttt{bin/hadoop-daemon.sh --config \ldots/conf start datanode}.

\heading{F. Better view of hadoop process}

\noindent Suppose the master machine is ml64-20.cms.waikato.ac.nz, and the port
for jobtracker is 50030. Then log in to ml64-20, start a browser, and type in 
http://ml64-20.cms.waikato.ac.nz:50030. You will have an overall view of the
cluster, about how many nodes available, what is current job status etc.



\chapter{Summary}
It is strongly recomended that using Hadoop Gui Experimenter from ADAMS to run
experiments. The program has been designed to automatically remove all the
unnecessary files that were generated during a hadoop process, except for the final
output files and the jar file.\\

\noindent Multiple experiments can be run simutaneously on a cluster, or on
several clusters. As long as you provide different output file names to those
experiments, it shouldn't be a problem.\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\input{bibliography} 
\begin{thebibliography}{999}
	% to make the bibliography appear in the TOC
	\addcontentsline{toc}{chapter}{Bibliography}

    % references
	\bibitem{adams}
		\textit{ADAMS} -- Advanced Data mining and Machine learning System \\
		\url{http://adams.cms.waikato.ac.nz/}{}
		\url{http://adams.cms.waikato.ac.nz/}{}

	\bibitem{weka}
	 	Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter
	 	Reutemann, Ian H. Witten (2009); \textit{The WEKA Data Mining Software: An
	 	Update}; SIGKDD Explorations, Volume 11, Issue 1. \\
		\url{http://www.cs.waikato.ac.nz/ml/weka/}{}

	\bibitem{wekabook}
		Ian H. Witten, Eibe Frank, Mark A. Hall (2011); \textit{Data Mining: Practical
		Machine Learning Tools and Techniques}; Third Edition; Morgan Kaufmann; ISBN
		978-0-12-374856-0 \\
		\url{http://www.cs.waikato.ac.nz/ml/weka/book.html}{}

	\bibitem{hadoop}
	 	\textit{Apache Hadoop} -- Open-source software for reliable, scalable,
	 	distributed computing \\
		\url{http://hadoop.apache.org/}{}
    \bibitem{hadoopBook}
    	Tom White (2009); \textit{Hadoop: The Definitive Guide}; First
    	Edition;O'Reilly; ISBN 978-0-596-52197-4\\
    	\url{http://books.google.com/books?id=Nff49D7vnJcC}{}
    \bibitem{hadoopBook2}
    	Jason Venner(2009); \textit{Pro Hadoop}; APress; ISBN 978-1-4302-1942-2\\
    	\url{http://books.google.com/books?id=H3mvcxPeUfwC}{}
\end{thebibliography}
\end{document}
